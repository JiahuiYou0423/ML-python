# -*- coding: utf-8 -*-
"""random forest and ensemlble learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u7-73FEYBOV52eayKp4m6QffEQBhCAqQ
"""

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt



"""# Ensemble learning
- hard vote: use the majority result
- Soft vote: use the highest possibility value
"""

from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons
x, y = make_moons(n_samples = 500, noise =0.3, random_state=40)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=40)
plt.plot(x[:,0][y==0], x[:,1][y==0], 'yo', alpha=0.6)
plt.plot(x[:,0][y==1], x[:,1][y==1], 'bs', alpha=0.6)
plt.show()



"""# Voting strategy
## - Hard voting
"""

from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

log_clf = LogisticRegression(random_state=42)
rnd_clf = RandomForestClassifier(random_state=42)
svc_clf = SVC(random_state=42)

voting_clf = VotingClassifier(estimators=[('lc',log_clf),('rd',rnd_clf),('svc_clf',svc_clf)],voting='hard')

voting_clf.fit(x_train, y_train)

from sklearn.metrics import accuracy_score
for clf in (log_clf, rnd_clf, svc_clf, voting_clf):
  clf.fit(x_train, y_train)
  y_pred = clf.predict(x_test)
  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))

"""## - Soft voting

  Each classfier need to get the probability
"""

from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

log_clf = LogisticRegression(random_state=42)
rnd_clf = RandomForestClassifier(random_state=42)
svc_clf = SVC(probability=True,random_state=42)
voting_clf = VotingClassifier(estimators=[('lc',log_clf),('rd',rnd_clf),('svc_clf',svc_clf)],voting='soft')

from sklearn.metrics import accuracy_score
for clf in (log_clf, rnd_clf, svc_clf, voting_clf):
  clf.fit(x_train, y_train)
  y_pred = clf.predict(x_test)
  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))

"""# Bagging"""

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(),
          n_estimators=500,
          max_samples=50,
          bootstrap=True,
          n_jobs = -1,
          random_state =42)
bag_clf.fit(x_train,y_train)
y_pred = bag_clf.predict(x_test)
accuracy_score(y_test, y_pred)

dec_clf = DecisionTreeClassifier(random_state=42)
dec_clf.fit(x_train,y_train)                                
y_pred_tree = dec_clf.predict(x_test)
accuracy_score(y_test, y_pred_tree)

"""# decision surface

- with
"""

from matplotlib.colors import ListedColormap
def plot_decision_boundary(clf, x, y, axes=[-1.5, 2.5, -1,1.5], alpha=0.5, contour=True):
  x1s = np.linspace(axes[0], axes[1], 100)
  x2s = np.linspace(axes[2], axes[3], 100)
  x1, x2 = np.meshgrid(x1s,x2s)
  x_new = np.c_[x1.ravel(),x2.ravel()]
  y_pred = clf.predict(x_new).reshape(x1.shape)
  if contour:
    custom_cmap2 = ListedColormap(['#800080','#0000FF','#00FFFF'])
    plt.contour(x1,x2,y_pred,cmap = custom_cmap2, alpha=0.8)
  plt.plot(x[:,0][y==0], x[:,1][y==0], 'yo', alpha=0.6)
  plt.plot(x[:,0][y==1], x[:,1][y==1], 'bs', alpha=0.6)
  plt.axis(axes)
  plt.xlabel('x1')
  plt.xlabel('x2')

plt.figure(figsize = (12,5))
plt.subplot(121)
plot_decision_boundary(bag_clf,x,y)
plt.title('Decision tree with bagging')
plt.subplot(122)
plot_decision_boundary(dec_clf,x=x,y=y)
plt.title('decision tree')

"""# Out of Bag strategy"""

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(),
          n_estimators=500,
          max_samples=50,
          bootstrap=True,
          n_jobs = -1,
          random_state =42,
          oob_score=True)
bag_clf.fit(x_train, y_train)

bag_clf.oob_score_

y_pred = bag_clf.predict(x_test)
accuracy_score(y_test,y_pred)

bag_clf.oob_decision_function_



"""# random forest """

from sklearn.ensemble import RandomForestClassifier
rf_clf = RandomForestClassifier()
rf_clf.fit(x_train, y_train)

"""# Feature importance """

from sklearn.datasets import load_iris
iris = load_iris()
rf_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)
rf_clf.fit(iris['data'], iris['target'])
for name, score in zip(iris['feature_names'], rf_clf.feature_importances_):
  print(name, score)

"""## Mnist dataset feature importance"""

from sklearn.datasets import fetch_openml
X, y = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)

rf_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)
rf_clf.fit(X,y)

rf_clf.feature_importances_.shape

def plot_digit(data):
  image = data.reshape(28,28)
  plt.imshow(image, cmap=matplotlib.cm.hot)
  plt.axis('off')

plot_digit(rf_clf.feature_importances_)
char=plt.colorbar(ticks=[rf_clf.feature_importances_.min(),rf_clf.feature_importances_.max()])
char.ax.set_yticklabels(["not importance", 'very important'])

"""# Boosting 
## AdaBoost
"""

m =len(x_train)
from sklearn.svm import SVC
plt.figure(figsize=(12,4))
for subplot, learning_rate in ((121,1), (122,0.5)):
  sample_weights = np.ones(m)
  plt.subplot(subplot)
  for i in range(5):
    svm_clf = SVC(kernel='rbf', C=0.05, random_state=42)
    svm_clf.fit(x_train, y_train, sample_weight = sample_weights)
    y_pred = svm_clf.predict(x_train)
    sample_weights[y_pred != y_train] *=(1+learning_rate)
    plot_decision_boundary(svm_clf,x_train,y_train)
    plt.title("learning_rate={}".format(learning_rate))
plt.show()

from sklearn.ensemble import AdaBoostClassifier
adb_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
                             n_estimators=200, 
                             learning_rate=0.5,
                             random_state=42)
adb_clf.fit(x_train, y_train)
plot_decision_boundary(adb_clf,x_train, y_train)

"""# Gradient boosting"""

np.random.seed(45)
x = np.random.rand(100,1)-0.5
y = 3*x[:,0]**2 + 0.05*np.random.randn(100)

from sklearn.tree import DecisionTreeRegressor
dec_reg = DecisionTreeRegressor()

dec_reg.fit(x,y)

y2 = y-dec_reg.predict(x)
dec_reg2 =DecisionTreeRegressor()
dec_reg2.fit(x,y2)

y3 = y2-dec_reg2.predict(x)
dec_reg3 =DecisionTreeRegressor()
dec_reg3.fit(x,y3)

x_new = np.array([[0.8]])
y_pred = sum(tree.predict(x_new) for tree in (dec_reg,dec_reg2,dec_reg3))
y_pred

""" - gradient boosting package: XGboost, lightGBM"""

from sklearn.ensemble import GradientBoostingRegressor
gbdt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)
gbdt.fit(x, y)

from sklearn.ensemble import GradientBoostingRegressor
gbdt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=0.1, random_state=42)
gbdt_slow.fit(x, y)

from sklearn.ensemble import GradientBoostingRegressor
gbdt_3 = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)
gbdt_3.fit(x, y)

x1 = np.linspace([-0.5], [0.5], 100)
plt.figure(figsize = (11,4))
plt.subplot(121)
plt.plot(x, y ,'b.')
plt.plot(x1,gbdt.predict(x1), 'r-', label='learning_rate 1')
plt.axis([-0.5,0.5,-0.1,0.8])
plt.title("learning_rate is {}".format(gbdt.learning_rate))
plt.subplot(122)
plt.plot(x, y ,'b.')
plt.plot(x1,gbdt_slow.predict(x1), 'r-', label='learning_rate 0.1')
plt.axis([-0.5,0.5,-0.1, 0.8])
plt.title("learning_rate is {}".format(gbdt_slow.learning_rate))
plt.show()

x1 = np.linspace([-0.5], [0.5], 100)
plt.figure(figsize = (11,4))
plt.subplot(121)
plt.plot(x, y ,'b.')
plt.plot(x1,gbdt.predict(x1), 'r-', label='learning_rate 1')
plt.axis([-0.5,0.5,-0.1,0.8])
plt.title("Tree number is {}".format(gbdt.n_estimators_))
plt.subplot(122)
plt.plot(x, y ,'b.')
plt.plot(x1,gbdt_3.predict(x1), 'r-', label='learning_rate 0.1')
plt.axis([-0.5,0.5,-0.1, 0.8])
plt.title("tree number is {}".format(gbdt_3.n_estimators_))
plt.show()

"""# Early stopping strategy"""

from sklearn.metrics import mean_squared_error
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)
grbt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)
grbt.fit(x_train, y_train)

errors = [mean_squared_error(y_test, y_pred) for y_pred in grbt.staged_predict(x_test)]
best_n_estimators = np.argmin(errors)
grbt_best =  GradientBoostingRegressor(max_depth=2, n_estimators=best_n_estimators , random_state=42)
grbt_best.fit(x_train, y_train)

min_error = np.min(errors)
min_error

plt.figure(figsize=(11,4))
plt.subplot(121)
plt.plot(errors, 'b.')
plt.plot([best_n_estimators, best_n_estimators], [0,min_error], 'k--')
plt.plot([0,120], [min_error, min_error], 'k--')
plt.title('val error')
plt.axis([0,120,0,0.01])
plt.subplot(122)
plt.plot(x,y,'b.')
x1 = np.linspace([-0.5], [0.5], 100)
plt.plot(x1, grbt_best.predict(x1), 'r-')
plt.title("best method with estimator {}".format(grbt_best.n_estimators_))

"""- Early stopping strategy"""

grbt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)  #use warm start method
min_val_error = float('inf')
error_going_up = 0
for n_estimators in range(1,120):
  grbt.n_estimators = n_estimators
  grbt.fit(x_train, y_train)
  y_pred = grbt.predict(x_test)
  val_error = mean_squared_error(y_test, y_pred)
  if val_error<min_val_error:
    min_val_error = val_error
    error_going_up = 0
  else:
    error_going_up += 1
    if error_going_up == 5:
      break

print(grbt.n_estimators_)

"""# Stacking"""

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
mnist = fetch_openml("mnist_784", version=1,as_frame=True)

x_train, x_test, y_train, y_test = train_test_split(mnist.data, mnist.target, train_size=0.8, random_state=42)

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.svm import LinearSVC
from sklearn.neural_network import MLPClassifier
rdm_clf = RandomForestClassifier(random_state=42)
ext_clf = ExtraTreesClassifier(random_state=42)
svc_clf = LinearSVC(max_iter=1500,random_state=42)
mlp_clf = MLPClassifier(random_state=42)
estimators = [rdm_clf, ext_clf, svc_clf, mlp_clf]

for estimator in estimators:
  print("training the {}".format(estimator))
  estimator.fit(x_train, y_train)

"""For stacking method, use the first stage result as the input for the second stage"""

type(enumerate(estimators))

x_test_pred = np.empty((len(x_test), len(estimators)), dtype=np.float32)
for index, estimator in enumerate(estimators):
  x_test_pred[:,index] = estimator.predict(x_test)

rnd_forest_blender = RandomForestClassifier(n_estimators=200, random_state=42, oob_score=True)
rnd_forest_blender.fit(x_test_pred, y_test)
rnd_forest_blender.oob_score_