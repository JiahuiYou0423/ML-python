# -*- coding: utf-8 -*-
"""Logistic regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hXmXneHXxQIGqPtlgIj3ir7K-NuDAhzz
"""

import numpy as np
import os 
import matplotlib 
import matplotlib.pyplot as plt
import pandas as pd
np.random.seed(42)

"""Sigmoid function give the number in range [0,1] and help us finish the binary classification problem """

from sklearn import datasets
iris = datasets.load_iris()
[iris.keys()]

x = iris['data'][:,3:]
y = (iris['target']==2).astype(np.int) # there are three classes in the label, first made label==2 as 1, others as 0
y

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(x,y)



x_new = np.linspace(0,3,1000).reshape(-1,1)
y_pred_prob = log_reg.predict_proba(x_new) #return the probability
y_pred_prob

"""Draw an arrow"""

plt.figure(figsize=(12,5))
decision_boundary = x_new[y_pred_prob[:,1]>=0.5][0]
plt.plot([decision_boundary, decision_boundary],[-1,2], 'k:', linewidth=2)
# plt.axis([0,1,0,3])
plt.plot(x_new,y_pred_prob[:,1], 'g-', label='Virginica')
plt.plot(x_new,y_pred_prob[:,0], 'b--', label='not Virginica')
plt.arrow(decision_boundary,0.08,-0.3, 0, head_width=0.05, fc='b',ec='b')
plt.arrow(decision_boundary,0.92,0.3, 0, head_width=0.05, fc='g',ec='g')
plt.axis([0,3, -0.02, 1.02])
plt.text(decision_boundary+0.02, 0.15, "Decision Boundary", fontsize=16, color = 'k', ha='center')
plt.xlabel("Peta width(cm)", fontsize=16)
plt.ylabel("y_proba", fontsize=16)
plt.legend(loc='center left', fontsize=16)

print(help(plt.arrow))

x =iris['data'][:,(2,3)]
y=(iris['target']==2).astype(np.int)

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(C=1000)
log_reg.fit(x,y)

"""create the location data

exp:
"""

x0,x1=np.meshgrid(np.linspace(1,2,2).reshape(-1,1), np.linspace(10,20,3).reshape(-1,1))

x0

x1

np.c_[x0.ravel(),x1.ravel()]  #merge x0,x1

x[:,0].min(), x[:,0].max()

x[:,1].min(), x[:,1].max()

x0,x1=np.meshgrid(np.linspace(2.9,7,500).reshape(-1,1), np.linspace(0.8,2.7,200).reshape(-1,1)) #choose the random number can cover the min,max of x

x_new = np.c_[x0.ravel(),x1.ravel()]
x_new

y_pred = log_reg.predict_proba(x_new)

plt.figure(figsize=(10,4))
plt.plot(x[y==0, 0], x[y==0,1], 'bs')
plt.plot(x[y==1, 0], x[y==1,1], 'g^')

#contour
zz = y_pred[:,1].reshape(x0.shape)
contour = plt.contour(x0,x1,zz,cmap=plt.cm.brg)
plt.clabel(contour) #add number on the contour
plt.axis([2.9, 7, 0.8,2.7])
plt.text(3.5, 1.5, 'Not Vir', fontsize=16, color='b')
plt.text(6.5, 2.5, 'Vir', fontsize=16, color='g')

"""# Multi-calssification problem-Softmax
softmax normalize the probability -first enlarge the difference and then normalize the probability

"""

x=iris['data'][:,(2,3)]
y=iris['target']
softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs')
softmax_reg.fit(x,y)

y_predict = softmax_reg.predict([[2,4]])
y_predict

softmax_reg.predict_proba([[2,4]])

x0,x1=np.meshgrid(np.linspace(0,8,500).reshape(-1,1), np.linspace(0,3.5,200).reshape(-1,1))
x_new = np.c_[x0.ravel(),x1.ravel()]

y_proba = softmax_reg.predict_proba(x_new)
y_predict = softmax_reg.predict(x_new)
zzl = y_proba[:,1].reshape(x0.shape)
zz = y_predict.reshape(x0.shape)
plt.figure(figsize=(10,4))
plt.plot(x[y==2, 0], x[y==2,1], 'bs', label='iris-virg')
plt.plot(x[y==1, 0], x[y==1,1], 'g^', label='iris-vers')
plt.plot(x[y==0, 0], x[y==0,1], 'yo', label='iris-Seto')
from matplotlib.colors import ListedColormap
custom_camp = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])
plt.contourf(x0,x1,zz,cmap=custom_camp)
contour = plt.contour(x0,x1,zzl,camp=plt.cm.brg)
plt.xlabel("petal length", fontsize=14)
plt.ylabel("letal width", fontsize=14)
plt.clabel(contour, inline=1, fontsize=12) #add number on the contour
plt.axis([0, 7, 0,3.5])
plt.legend(loc='center left')
plt.text(3.5, 1.5, 'Not Vir', fontsize=16, color='b')
plt.text(6.5, 2.5, 'Vir', fontsize=16, color='g')

